Thoughts
=======================

Implementation
---------
* geopy library is used for distance calculation between points.
* Geohash values are used as identifiers for performing quick proximity tests
* Trie data structure is used as a container with geohash values as keys
* proximityhash library is used to generate geohash values within a radius
* set data structure is used in order to check for intersections between collections (ie: tags)
* When the api gets initialized csv files are used to form data strictures which are used during searches. This is a one time operation, which does not reoccur for each search.
* General flow of a search is as the following (in terms of specific concrete classes):
    * the search api handler calls SearchProvider's search method
        * SearchProvider calls locator's (HybridLocator) search method to retrieve shops and their locations within the area of interest.
            * HybridLocator checks if the area of interest is within limited regions where geohash values are not fit for proximity searches. If so, falls back to BasicLocator's search functionality. If not, uses GeohashLocator's search method.
        * After retrieving shops and locations which are within the search radius, shops are further filtered according to their tags. (If the caller of the api has provided any tags)
        * Then, top N products are selected from most popular products of shops passing through previous filters
    * Finally, the resulting products and their corresponding shop locations are sent back to the consumer of the api.




Performance Considerations
---------
* Searches in trie data structure are highly performant with O(m) time complexity, where m is the length of the average geohash. The maximum lenght of geohashes used here are 12 characters, therefore there are significant performance gains compared to searches with 0(n) or O(log n) time complexity (where n is the number of elements).

Tests
---------
* Personal lack of knowledge about python's ecosystem resulted in tests without mocks (need to study mocking frameworks in python).
* Regarding the scope of this excercise, tests about reading csv input are left out since they are internally generated by the system. In real life these tests should be included too.


Improvements/Thoughts (Design, performance, features, etc.)
---------
* If keeping the data in memory is considered to be un feasible, csv files with ordered keys can be used.
* At its current state the in-memory data is not optimized. There are pieces of data which are not required but still held in memory. These can be removed to reduce memory consumption.
* Reducing geohash length would also save both memory and processing time by reducing the trie's size. However, too much reduction in geohash length will manifest itself in noticeably coarse search results.
* If resources of a single machine are reached, multiple instances of the same service may be employed with a form of load balancing according to search's computed geohash.
* Logging functionalty should be added before going to production.
* async processing should be considered especially in case a decision is taken to use disk based data access instead of the memory based one employed here.
* Usually it is a good use a reverse proxy (nginx etc.) instead of exposing the service as it is.
* CSV file format and data integrity verification such as duplicate key checks should be considered depending on the source of the csv file.
* Currently entity/model names and properties are bound to fileds in the first row of csv files. Changes in those calues will require code change. Fields can be mapped in a central location.
* My lack of knowledge about python has not been a major detriment in terms of cohesion. However, on the issue of class coupling, due to lack of strong typing (no explicity defined interfaces for class members, parameters etc), at times I was not sure if a class was being coupled to an implementation or not.
* Dependency injection was not considered due to lack of knowledge on DI frameworks in python